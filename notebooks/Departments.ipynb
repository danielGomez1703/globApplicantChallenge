{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c57fcdf-0bd4-4684-b473-b47a9d72d29f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "import os\n",
    "import logging\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c2b3e8b-0d53-424b-a529-656796a42afe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"appGlob\") \\\n",
    "    .config(\"spark.jars\",\"../controller/postgresql-42.7.2.jar\")\\\n",
    "    .config('spark.driver.extraClassPath', '../controller/postgresql-42.7.2.jar') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ca208da-9de7-4341-b441-0728fa3a0ffe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'database'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m config \u001b[38;5;241m=\u001b[39m configparser\u001b[38;5;241m.\u001b[39mConfigParser()\n\u001b[0;32m      2\u001b[0m config\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./properties.conf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m database_config \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatabase\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\configparser.py:979\u001b[0m, in \u001b[0;36mRawConfigParser.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    977\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m    978\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_section \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_section(key):\n\u001b[1;32m--> 979\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[0;32m    980\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_proxies[key]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'database'"
     ]
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read(\"../properties.conf\")\n",
    "database_config = config[\"database\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd8eb4e-bb48-4efd-9682-fa9fa6f43515",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#log\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"ReadCSV\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9d1339-0bab-4be4-8cce-e559489b0050",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD_FILE\n",
    "filePath = \"../inputs/\"\n",
    "fileType = \"departments.csv\"\n",
    "# Define the schema\n",
    "dp_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"department\", StringType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248722e8-bf08-4d68-a07c-f7f924ea98f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try: \n",
    "    df = spark.read.csv(filePath+fileType,header=False, inferSchema=False, schema=dp_schema ,sep=\",\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(\"Error: %s\", str(e), exc_info=True)\n",
    "    df = spark.createDataFrame([], schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0d5a8d-501a-4900-ba96-9a1dbccb28b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger.info(\"Dataframe schema for department table:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0f1dbe-8a53-4592-a8e8-09b64da363da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ca7120-9f78-45df-a9a8-823daa6768bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try :\n",
    "    df.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"url\", database_config[\"url\"]) \\\n",
    "    .option(\"dbtable\", \"departments\") \\\n",
    "    .option(\"user\", database_config[\"user\"]) \\\n",
    "    .option(\"password\", database_config[\"password\"]) \\\n",
    "    .option(\"driver\", database_config[\"driver\"]) \\\n",
    "    .save()\n",
    "except Exception as e:\n",
    "    logger.error(\"Erro: review eschemas or datatypes %s\", str(e), exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cc104f-b2ae-471b-b6e3-b88f9f5e5d66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
